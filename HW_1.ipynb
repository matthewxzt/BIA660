{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Assignment-1\" data-toc-modified-id=\"Assignment-1-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><center>Assignment 1</center></a></span><ul class=\"toc-item\"><li><span><a href=\"#Q1.-Define-a-function-to-create-vocabulary-dictionary-for-an-input-document\" data-toc-modified-id=\"Q1.-Define-a-function-to-create-vocabulary-dictionary-for-an-input-document-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Q1. Define a function to create vocabulary dictionary for an input document</a></span></li><li><span><a href=\"#Q2.-Define-a-class-to-analyze-a-document-##-(5-points)\" data-toc-modified-id=\"Q2.-Define-a-class-to-analyze-a-document-##-(5-points)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Q2. Define a class to analyze a document ## (5 points)</a></span></li><li><span><a href=\"#Q3.-(Bonus)-Create-Bigrams-from-a-document-##-(3-points)\" data-toc-modified-id=\"Q3.-(Bonus)-Create-Bigrams-from-a-document-##-(3-points)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Q3. (Bonus) Create Bigrams from a document ## (3 points)</a></span></li><li><span><a href=\"#Submission-Guideline##\" data-toc-modified-id=\"Submission-Guideline##-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Submission Guideline##</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are required to define a class to analyze an article. An sample article is provided. However, your class should be able to analyze any article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to create vocabulary dictionary for an input document\n",
    " - Define a function named `tokenize` which does the following:\n",
    "     * take a document (i.e. a string) as an input \n",
    "     * split the document into a list of tokens by space (including tab, i.e. `\\t`, newline, i.e. `\\n`)\n",
    "     * remove leading or trailing spaces around each token. \n",
    "     * remove leading or trailing punctuations around each token. However, punctuations are allowed to appear in the middle of a token (e.g. life-saving, didn't).\n",
    "     * remove empty tokens, i.e. a token has a least one character\n",
    "     * convert all tokens into lower case\n",
    "     * return the remaining tokens as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T03:09:51.844838Z",
     "start_time": "2021-09-07T03:09:51.837839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "9, 11\n"
     ]
    }
   ],
   "source": [
    "# hint1: If needed, you can use the following to retrieve the list of punctions\n",
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "# hint 2: you can use the following to remove leading or trailing punctuations around a token\n",
    "token = ' (9, 11). '\n",
    "print(token.strip(string.punctuation+ ' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T03:09:51.853838Z",
     "start_time": "2021-09-07T03:09:51.846836Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def tokenize(doc):\n",
    "    tokens=doc.split()\n",
    "    tokenize=[token.strip() for token in tokens]\n",
    "    tokenize=[token.strip(string.punctuation+ ' ') for token in tokens]\n",
    "    tokenize=[token.strip() for token in tokens if len(token.strip())>1]\n",
    "    tokenize=[token.lower() for token in tokens]\n",
    "    return tokenize\n",
    "    # add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T03:09:51.859839Z",
     "start_time": "2021-09-07T03:09:51.855839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'employs', 'computational', 'techniques', 'for', 'the', 'purpose', 'of', 'learning,', 'understanding,', 'and', 'producing', 'human', 'language', 'content.', 'early', 'computational', 'approaches', 'to', 'language', 'research', 'focused', 'on', 'automating', 'the', 'analysis', 'of', 'the', 'linguistic', 'structure', 'of', 'language', 'and', 'developing', 'basic', 'technologies', 'such', 'as', 'machine', 'translation,', 'speech', 'recognition,', 'and', 'speech', 'synthesis.', 'today’s', 'researchers', 'refine', 'and', 'make', 'use', 'of', 'such', 'tools', 'in', 'real-world', 'applications,', 'creating', 'spoken', 'dialogue', 'systems', 'and', 'speech-to-speech', 'translation', 'engines,', 'mining', 'social', 'media', 'for', 'information', 'about', 'health', 'or', 'finance,', 'and', 'identifying', 'sentiment', 'and', 'emotion', 'toward', 'products', 'and', 'services.', 'we', 'describe', 'successes', 'and', 'challenges', 'in', 'this', 'rapidly', 'advancing', 'area.']\n"
     ]
    }
   ],
   "source": [
    "doc = '''Natural language processing employs computational techniques for the purpose of learning,\n",
    "understanding, and producing human language content. Early computational approaches to\n",
    "language research focused on automating the analysis of the linguistic structure of language\n",
    "and developing basic technologies such as machine translation, speech recognition, and speech\n",
    "synthesis. Today’s researchers refine and make use of such tools in real-world applications,\n",
    "creating spoken dialogue systems and speech-to-speech translation engines, mining social\n",
    "media for information about health or finance, and identifying sentiment and emotion toward\n",
    "products and services. We describe successes and challenges in this rapidly advancing area. \n",
    "         '''\n",
    "\n",
    "print(tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Define a class to analyze a document ## (5 points)\n",
    "\n",
    "\n",
    " - Define a new class called `Doc_Analyzer` as follows: \n",
    "    - it has two attributes:\n",
    "        - `tokens`: store the list of tokens as in the order they appear in the document\n",
    "        - `vocab`: a dictionary to store the count of each token  \n",
    "    - it has an `__init__` function which \n",
    "       - takes a document (i.e. a string) as an input\n",
    "       - calls the `tokenize` function that you defined in Q1 to get the list of tokens\n",
    "       - store the returned tokens to attribute `tokens`\n",
    "       - count each token, and save the frequency of each token into attribute `vocab` such that\n",
    "           - each key is a token\n",
    "           - the value of each key is the count of the token         \n",
    "    - it has a function named `get_doc_length` which returns the total number of tokens in the document\n",
    "    - it has a function named `get_topN` which \n",
    "        - takes a number `N` as an input\n",
    "        - finds the most frequent `N` words and their counts in the document\n",
    "        - returns the words and their counts as a list of tuples\n",
    "        \n",
    "- Question: What words are frequent? Are you able to get a good idea about the article based on the frequent words? Write your analysis as a pdf file and submit to Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T03:09:51.894839Z",
     "start_time": "2021-09-07T03:09:51.884839Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Doc_Analyzer(object):\n",
    "     \n",
    "    def __init__(self,vocab):\n",
    "        tokens=tokenize(doc)\n",
    "        self.vocab={token:tokens.count(token) for token in set(tokens)}\n",
    "            \n",
    "    def get_doc_length(self):\n",
    "        \n",
    "        tokens=tokenize(doc)\n",
    "        get_doc_length=len(tokens)\n",
    "        return get_doc_length\n",
    "   \n",
    "    def  get_topN(self,N):\n",
    "        from collections import Counter         \n",
    "        tokens=tokenize(doc)\n",
    "        self.get_topN=Counter(tokens).most_common(N)\n",
    "        return self.get_topN\n",
    "    # add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T03:09:51.903838Z",
     "start_time": "2021-09-07T03:09:51.896838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'successes': 1, 'spoken': 1, 'rapidly': 1, 'machine': 1, 'developing': 1, 'such': 2, 'emotion': 1, 'computational': 2, 'today’s': 1, 'make': 1, 'speech-to-speech': 1, 'techniques': 1, 'media': 1, 'research': 1, 'this': 1, 'for': 2, 'to': 1, 'researchers': 1, 'use': 1, 'in': 2, 'processing': 1, 'dialogue': 1, 'refine': 1, 'challenges': 1, 'recognition,': 1, 'understanding,': 1, 'or': 1, 'natural': 1, 'speech': 2, 'tools': 1, 'about': 1, 'the': 3, 'describe': 1, 'advancing': 1, 'finance,': 1, 'content.': 1, 'focused': 1, 'as': 1, 'synthesis.': 1, 'applications,': 1, 'technologies': 1, 'sentiment': 1, 'analysis': 1, 'toward': 1, 'language': 4, 'approaches': 1, 'of': 4, 'automating': 1, 'identifying': 1, 'area.': 1, 'systems': 1, 'employs': 1, 'engines,': 1, 'structure': 1, 'linguistic': 1, 'human': 1, 'services.': 1, 'social': 1, 'translation,': 1, 'information': 1, 'and': 9, 'creating': 1, 'translation': 1, 'health': 1, 'products': 1, 'we': 1, 'real-world': 1, 'producing': 1, 'learning,': 1, 'mining': 1, 'early': 1, 'on': 1, 'basic': 1, 'purpose': 1}\n",
      "95\n",
      "[('and', 9), ('language', 4), ('of', 4), ('the', 3), ('computational', 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "analyzer = Doc_Analyzer(doc)\n",
    "\n",
    "print(analyzer.vocab)\n",
    "\n",
    "print(analyzer.get_doc_length())\n",
    "\n",
    "print(analyzer.get_topN(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. (Bonus) Create Bigrams from a document ## (3 points)\n",
    "\n",
    "A n-gram is n consecutive tokens in a document. Phrases (e.g. data management) are usually frequent n-grams. In the class `Doc_Analyzer`, let's add a function to find phrases.\n",
    " - Create a new function for the class called `get_ngram` as follows :\n",
    "     * take two parameters:\n",
    "         - `n_gram`: indicate if you want to retrieve phrases in the form of unigram (`n_gram = 1`), bigram (`n_gram = 2`), trigram (`n_gram = 3`) and so on\n",
    "         - `N`: top N phrases are returned\n",
    "     * slice the `tokens` to get any consecutive `n_gram` tokens if `n_gram > 1`;  if `n_gram == 1` , call `get_topN` as you define in Q2; if `n_gram <1`, `raise ValueError('n_gram should not less than 1')`\n",
    "     * count the frequency of each unique phrase\n",
    "     * return top N phrases and their counts as a list of tuples\n",
    "\n",
    "Question: Are you able to find good phrases from the top N n-grams? Write down your analysis in a document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T03:13:23.517947Z",
     "start_time": "2021-09-07T03:13:23.363949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 194), ('of', 178), ('and', 166), ('to', 125), ('in', 106)]\n",
      "None\n",
      "[(\"['such', 'as']\", 32), (\"['of', 'the']\", 18), (\"['can', 'be']\", 18), (\"['in', 'the']\", 18), (\"['social', 'media']\", 10)]\n",
      "None\n",
      "[(\"['as', 'well', 'as']\", 8), (\"['the', 'development', 'of']\", 7), (\"['of', 'human', 'language']\", 4), (\"['spoken', 'dialogue', 'systems']\", 3), (\"['mining', 'social', 'media']\", 3)]\n",
      "None\n",
      "n_gram should not less than 1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Try the long document: Hirschberg, J. and Manning, C.D., 2015. Advances in natural language processing. Science, 349(6245), pp.261-266.\n",
    "doc = open(\"hw1_test_doc.txt\",\"r\", encoding='utf-8').read()\n",
    "\n",
    "from collections import Counter\n",
    "class Doc_Analyzer():\n",
    "    def __init__(self,vocab):\n",
    "        tokens = tokenize(doc)\n",
    "        dict = {} \n",
    "        for i in tokens:\n",
    "            dict[i] = dict.get(i, 0) + 1 \n",
    "            self.vocab = dict\n",
    "            \n",
    "    def get_doc_length(self):\n",
    "        return(len(tokenize(doc)))      \n",
    "    \n",
    "    def get_topN(self,n):\n",
    "        from collections import Counter         \n",
    "        tokens=tokenize(doc)\n",
    "        self.get_topN=Counter(tokens).most_common(n)\n",
    "        return self.get_topN\n",
    "\n",
    "    def get_ngram(self,n_gram,n):\n",
    "        tokens=tokenize(doc)\n",
    "        if n_gram > 1:\n",
    "           \n",
    "            tokens_1=[tokens[i:i+n_gram] for i in range(0,len(tokens))]\n",
    "            list2=[str(i) for i in tokens_1]\n",
    "            self.result=Counter(list2).most_common(n)\n",
    "            print(self.result )\n",
    "        elif n_gram == 1:\n",
    "            print(analyzer.get_topN(n))\n",
    "        elif n_gram < 1:\n",
    "            print('n_gram should not less than 1')\n",
    "    \n",
    "analyzer = Doc_Analyzer(doc)\n",
    "print(analyzer.get_ngram(1, 5))\n",
    "print(analyzer.get_ngram(2, 5))\n",
    "print(analyzer.get_ngram(3, 5))\n",
    "print(analyzer.get_ngram(0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guideline##\n",
    "- Following the solution template provided below. Use __main__ block to test your functions and class\n",
    "- Save your code into a python file (e.g. assign1.py) that can be run in a python 3 environment. In Jupyter Notebook, you can export notebook as .py file in menu \"File->Download as\".\n",
    "- Make sure you have all import statements. To test your code, open a command window in your current python working folder, type \"python assign1.py\" to see if it can run successfully.\n",
    "- For more details, check assignment submission guideline on Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T03:09:52.176838Z",
     "start_time": "2021-09-07T03:09:52.176838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Question 1\n",
      "['natural', 'language', 'processing', 'employs', 'computational', 'techniques', 'for', 'the', 'purpose', 'of', 'learning,', 'understanding,', 'and', 'producing', 'human', 'language', 'content.', 'early', 'computational', 'approaches', 'to', 'language', 'research', 'focused', 'on', 'automating', 'the', 'analysis', 'of', 'the', 'linguistic', 'structure', 'of', 'language', 'and', 'developing', 'basic', 'technologies', 'such', 'as', 'machine', 'translation,', 'speech', 'recognition,', 'and', 'speech', 'synthesis.', 'today’s', 'researchers', 'refine', 'and', 'make', 'use', 'of', 'such', 'tools', 'in', 'real-world', 'applications,', 'creating', 'spoken', 'dialogue', 'systems', 'and', 'speech-to-speech', 'translation', 'engines,', 'mining', 'social', 'media', 'for', 'information', 'about', 'health', 'or', 'finance,', 'and', 'identifying', 'sentiment', 'and', 'emotion', 'toward', 'products', 'and', 'services.', 'we', 'describe', 'successes', 'and', 'challenges', 'in', 'this', 'rapidly', 'advancing', 'area.']\n",
      "\n",
      "Test Question 2\n",
      "vocabulary: \n",
      " {'natural': 1, 'language': 4, 'processing': 1, 'employs': 1, 'computational': 2, 'techniques': 1, 'for': 2, 'the': 3, 'purpose': 1, 'of': 4, 'learning,': 1, 'understanding,': 1, 'and': 9, 'producing': 1, 'human': 1, 'content.': 1, 'early': 1, 'approaches': 1, 'to': 1, 'research': 1, 'focused': 1, 'on': 1, 'automating': 1, 'analysis': 1, 'linguistic': 1, 'structure': 1, 'developing': 1, 'basic': 1, 'technologies': 1, 'such': 2, 'as': 1, 'machine': 1, 'translation,': 1, 'speech': 2, 'recognition,': 1, 'synthesis.': 1, 'today’s': 1, 'researchers': 1, 'refine': 1, 'make': 1, 'use': 1, 'tools': 1, 'in': 2, 'real-world': 1, 'applications,': 1, 'creating': 1, 'spoken': 1, 'dialogue': 1, 'systems': 1, 'speech-to-speech': 1, 'translation': 1, 'engines,': 1, 'mining': 1, 'social': 1, 'media': 1, 'information': 1, 'about': 1, 'health': 1, 'or': 1, 'finance,': 1, 'identifying': 1, 'sentiment': 1, 'emotion': 1, 'toward': 1, 'products': 1, 'services.': 1, 'we': 1, 'describe': 1, 'successes': 1, 'challenges': 1, 'this': 1, 'rapidly': 1, 'advancing': 1, 'area.': 1}\n",
      "doc length: \n",
      " 95\n",
      "top 5 words: \n",
      " [('and', 9), ('language', 4), ('of', 4), ('the', 3), ('computational', 2)]\n",
      "\n",
      "Test Question 3\n",
      "[(\"['such', 'as']\", 32), (\"['of', 'the']\", 18), (\"['can', 'be']\", 18), (\"['in', 'the']\", 18), (\"['social', 'media']\", 10)]\n",
      "top 5 bigrams: \n",
      " None\n",
      "[(\"['as', 'well', 'as']\", 8), (\"['the', 'development', 'of']\", 7), (\"['of', 'human', 'language']\", 4), (\"['spoken', 'dialogue', 'systems']\", 3), (\"['mining', 'social', 'media']\", 3)]\n",
      "top 5 trigrams: \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "# Structure of your solution to Assignment 1 \n",
    "\n",
    "import string\n",
    "\n",
    "# define your function and class here\n",
    "\n",
    "# best practice to test your class\n",
    "# if your script is exported as a module,\n",
    "# the following part is ignored\n",
    "# this is equivalent to main() in Java\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Question 1\n",
    "    doc = '''Natural language processing employs computational techniques for the purpose of learning,\n",
    "understanding, and producing human language content. Early computational approaches to\n",
    "language research focused on automating the analysis of the linguistic structure of language\n",
    "and developing basic technologies such as machine translation, speech recognition, and speech\n",
    "synthesis. Today’s researchers refine and make use of such tools in real-world applications,\n",
    "creating spoken dialogue systems and speech-to-speech translation engines, mining social\n",
    "media for information about health or finance, and identifying sentiment and emotion toward\n",
    "products and services. We describe successes and challenges in this rapidly advancing area. \n",
    "         '''\n",
    "\n",
    "    print(\"Test Question 1\")\n",
    "    print(tokenize(doc))\n",
    "    \n",
    "    \n",
    "    # Test Question 2\n",
    "    print(\"\\nTest Question 2\")\n",
    "    \n",
    "    analyzer = Doc_Analyzer(doc)\n",
    "\n",
    "    print(\"vocabulary: \\n\", analyzer.vocab)\n",
    "\n",
    "    print(\"doc length: \\n\", analyzer.get_doc_length())\n",
    "\n",
    "    print(\"top 5 words: \\n\", analyzer.get_topN(5))\n",
    "    \n",
    "    # Note that top words are not very meaningful. \n",
    "    # They are called stop words\n",
    "    \n",
    "    #3 Test Question 3\n",
    "    print(\"\\nTest Question 3\")\n",
    "    \n",
    "    doc = open(\"hw1_test_doc.txt\",\"r\", encoding='utf-8').read()\n",
    "\n",
    "    analyzer = Doc_Analyzer(doc)\n",
    "    \n",
    "    print(\"top 5 bigrams: \\n\", analyzer.get_ngram(2, 5))\n",
    "    print(\"top 5 trigrams: \\n\", analyzer.get_ngram(3, 5))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
