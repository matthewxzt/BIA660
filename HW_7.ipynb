{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW 7: Topic Modeling and Word Vectors</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you'll need the dataset that was used in HW 6:\n",
    "\n",
    "- hw_6_train.csv: This file contains a list of documents. It's used for training models\n",
    "- hw6_test.csv: This file contains a list of documents and their ground-truth labels. It's used for testing performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function `topic_modeling` as follows: \n",
    "- Take the following parameters:\n",
    "    - `train_text`: a list of train documents\n",
    "    - `test_text`: a list of test documents\n",
    "    - `test_label`: a list ground truth labels for the test documents\n",
    "    - `num_topics`: the number of topics\n",
    "    \n",
    "    \n",
    "- Cluster `train_text` into `num_topics` topics using LDA\n",
    "    - Fit and transform the `train_text` into word counts by a vectorizer. Then transform `test_text` by the fitted vectorizer.\n",
    "    - When generating counts, you need to tune parameters such as `stop_words` and `min_df` for better performance\n",
    "    \n",
    "    \n",
    "- Predict the topic mixture of each document in `test_text`, and then assign the document only to the topic with the `max probability`.\n",
    "- Apply `majority vote` rule to map the predicted topic IDs to `test_label`. Hint: \n",
    "    - Do not hardcode the mapping in your code because each run may give you a different mapping. You can use `idxmax` function of Pandas to generate the mapping dynamically \n",
    "    - You can use cross tabulation to map the clusters to ground truth labels. Check the class notes for details.\n",
    "- Print the classification report\n",
    "\n",
    "\n",
    "- Return the fitted lda model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function and compare the performance with what you achieved in HW6. Briefly analyze whether LDA can deliver better clustering for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "import nltk,string\n",
    "from gensim.models import word2vec\n",
    "from operator import itemgetter\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# add your import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Barguelonne (French: la Barguelonne) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conus nielsenae is a species of sea snail a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coleto Creek Reservoir is a reservoir on Cole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indian River is a 59.1-mile-long (95.1 km) tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Funtenseetauern is a 2579 m high border p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0   The Barguelonne (French: la Barguelonne) is a...\n",
       "1   Conus nielsenae is a species of sea snail a m...\n",
       "2   Coleto Creek Reservoir is a reservoir on Cole...\n",
       "3   Indian River is a 59.1-mile-long (95.1 km) tr...\n",
       "4   The Funtenseetauern is a 2579 m high border p..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Onychogomphus styx is a species of dragonfly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>The Bonnet Carré Spillway is a flood control ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Coleophora centaureivora is a moth of the Col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Paris Nogari (c. 1536–1601) was an Italian pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Blacktail Butte (7688 feet (2343 m)) is a but...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        description\n",
       "0      4   Onychogomphus styx is a species of dragonfly ...\n",
       "1      3   The Bonnet Carré Spillway is a flood control ...\n",
       "2      4   Coleophora centaureivora is a moth of the Col...\n",
       "3      1   Paris Nogari (c. 1536–1601) was an Italian pa...\n",
       "4      3   Blacktail Butte (7688 feet (2343 m)) is a but..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"hw6_train.csv\")\n",
    "train.head()\n",
    "\n",
    "test = pd.read_csv(\"hw6_test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(train_text, test_text, test_label, num_clusters):\n",
    "\n",
    "    lda = None\n",
    "    tf_vectorizer=None\n",
    "    stop = list(stopwords.words('english')) + ['said']\n",
    "    tf_vectorizer = CountVectorizer(min_df=3, stop_words=stop)\n",
    "    tf = tf_vectorizer.fit_transform(train_text)\n",
    "    test_tf = tf_vectorizer.transform(test_text)\n",
    "    corpus = gensim.matutils.Sparse2Corpus(tf, documents_columns=False)\n",
    "    id2word={idx:w for idx, w in enumerate(tf_vectorizer.get_feature_names())}\n",
    "    dictionary = corpora.Dictionary.from_corpus(corpus, id2word=id2word)\n",
    "    \n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus, num_topics = num_clusters, id2word=id2word, iterations=30)\n",
    "    test_corpus = gensim.matutils.Sparse2Corpus(test_tf,documents_columns=False)\n",
    "    predict = lda.get_document_topics(test_corpus)\n",
    "    a=[]\n",
    "    for i in range(0,500):\n",
    "        b=sorted(list(predict)[i], key=lambda x: x[1])[-1][0]\n",
    "        a.append(b)\n",
    "    predicted=list(a)\n",
    "    confusion_df = pd.DataFrame(list(zip(test_label.values, predicted)),columns = [\"label\", \"cluster\"])\n",
    "    crosstable=pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label)\n",
    "    mapping = dict(crosstable.idxmax(axis=1))\n",
    "    predicted_target = [mapping[i] for i in predicted]\n",
    "\n",
    "    print(metrics.classification_report(test_label, predicted_target))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.82      0.74       123\n",
      "           2       0.68      0.69      0.68       100\n",
      "           3       0.67      0.29      0.41       146\n",
      "           4       0.58      0.82      0.68       131\n",
      "\n",
      "    accuracy                           0.64       500\n",
      "   macro avg       0.65      0.66      0.63       500\n",
      "weighted avg       0.65      0.64      0.62       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = topic_modeling(train[\"description\"], \n",
    "                                   test[\"description\"], \n",
    "                                   test[\"label\"],\n",
    "                                   num_clusters=4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Supervised Sentiment Analysis Using Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you'll need dataset:\n",
    "- `hw7_train.csv`: dataset fro training\n",
    "- `hw7_test.csv`: dataset for test\n",
    "\n",
    "A snippet of the dataset is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Getting ready for college. I had a good sleep....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>We are having a party now to have all the fami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@marC0110 ummm.. i see you.. and i really wann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@saboteur1 Thanks for following  Much apprecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Why eat at home? Picnic plans for today are al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Getting ready for college. I had a good sleep....\n",
       "1      1  We are having a party now to have all the fami...\n",
       "2      0  @marC0110 ummm.. i see you.. and i really wann...\n",
       "3      1  @saboteur1 Thanks for following  Much apprecia...\n",
       "4      1  Why eat at home? Picnic plans for today are al..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"hw7_train.csv\")\n",
    "test = pd.read_csv(\"hw7_test.csv\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1: Train Word Vectors\n",
    "\n",
    "Write a function `train_wordvec(docs, vector_size)` as follows:\n",
    "- Take two inputs:\n",
    "    - `docs`: a list of documents\n",
    "    - `vector_size`: the dimension of word vectors\n",
    "- First tokenize `docs` into tokens\n",
    "- Use `gensim` package to train word vectors. Set the `vector size` and also carefully set other parameters such as `window`, `min_count` etc.\n",
    "- return the trained word vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordvec(docs, vector_size = 100):\n",
    "    \n",
    "    wv_model = None\n",
    "    sentences=[ [token.strip(string.punctuation).strip() for token in nltk.word_tokenize(doc.lower()) if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2] for doc in docs]\n",
    "    wv_model = word2vec.Word2Vec(sentences,vector_size=vector_size ,min_count=5, window=5, workers=4 )\n",
    "    # add your code\n",
    "    \n",
    "    return wv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = train_wordvec(train[\"text\"], vector_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2: Generate Vector Representation for Documents\n",
    "\n",
    "Write a function `generate_doc_vector(docs, wv_model)` as follows:\n",
    "- Take two inputs:\n",
    "    - `docs`: a list of documents, \n",
    "    - `wv_model`: trained word vector model. Set the default value to 100.\n",
    "- First tokenize each document `doc` in `docs` into tokens\n",
    "- For each token in `doc`, look up for its word vector in `wv_model`. Then the document vector (denoted as `d`) of `doc` can be calculated as the `mean of the word vectors of its tokens`, i.e. $d = \\frac{\\sum_{i \\in doc}{v_i}}{|doc|}$, where $v_i$ is the word vector of the i-th token.\n",
    "- Return the vector representations of all `docs` as a numpy array of shape `(n, vector_size)`, where `n` is the number of documents in `docs` and `vector_size` is the dimension of word vectors.\n",
    "\n",
    "\n",
    "Note: It may not be a good idea to represent a document as the mean of its word vectors. For example, if one word is positive and another is negative, the sum of the these two words may make the resulting vector is no longer sensitive to sentiment. You'll learn more advanced methods to generate document vector in deep learning courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc_vector(docs, wv_model):\n",
    "    \n",
    "    vectors = None\n",
    "    sentences=[ [token.strip(string.punctuation).strip() for token in nltk.word_tokenize(doc.lower()) if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2] for doc in docs]\n",
    "    d=[]\n",
    "    for sentence in sentences:\n",
    "        d2=[]\n",
    "        for word in sentence:\n",
    "            if word in wv_model.wv.key_to_index:\n",
    "                d1=wv_model.wv[word]\n",
    "            d2.append(d1)\n",
    "        d3=np.array(d2)\n",
    "        d4=np.mean(d3,axis=0)\n",
    "        d.append(d4)\n",
    "    vectors=np.array(d)\n",
    "    \n",
    "    # add your code\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = generate_doc_vector(train[\"text\"], wv_model)\n",
    "test_X = generate_doc_vector(test[\"text\"], wv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3: Put everything together\n",
    "\n",
    "\n",
    "Define a function `predict_sentiment(train_text, train_label, test_text, test_label, vector_size = 100)` as follows:\n",
    "\n",
    "- Take the following inputs:\n",
    "    - `train_text, train_label`: a list of documents and their labels for training\n",
    "    - `test_text, test_label`: a list of documents and their labels for testing,\n",
    "    - `vector_size`: the dimension of word vectors. Set the default value to 100.\n",
    "- Call `train_wordvec(docs, vector_size)` to train a word vector model using `train_text`\n",
    "- Call `generate_doc_vector(docs, wv_model)` to generate vector representations (denoted as `train_X`) for documents in `train_text`. \n",
    "- Call `generate_doc_vector(docs, wv_model)` to generate vector representations (denoted as `test_X`) for each document in `test_text`\n",
    "- Fit a linear SVM model using `train_X` and `train_label`\n",
    "- Predict the label for `test_X` and print out classification report for the testing subset.\n",
    "- This function has no return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(train_text, train_label, test_text, test_label, vector_size = 100):\n",
    "    wv_model = train_wordvec(train_text, vector_size = vector_size)\n",
    "    train_X = generate_doc_vector(train_text, wv_model)\n",
    "    test_X = generate_doc_vector(test_text, wv_model)\n",
    "    cls = svm.LinearSVC()\n",
    "    dtm=cls.fit(train_X,train_label)\n",
    "    predicted=cls.predict(test_X)\n",
    "    classification_report_result=classification_report(test_label,predicted)\n",
    "    print(classification_report_result)\n",
    "    # add your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.70      9968\n",
      "           1       0.70      0.72      0.71     10032\n",
      "\n",
      "    accuracy                           0.71     20000\n",
      "   macro avg       0.71      0.71      0.71     20000\n",
      "weighted avg       0.71      0.71      0.71     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthewxzt\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(train[\"text\"], train[\"label\"],\\\n",
    "                  test[\"text\"], test[\"label\"],\\\n",
    "                  vector_size = 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
