{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE9WGE_m-4J-"
   },
   "source": [
    "# HW 4: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKVEJOmf-4KC"
   },
   "source": [
    "## Q1: Extract data using regular expression\n",
    "Suppose you have scraped the text shown below from an online source. \n",
    "Define a `extract` function which:\n",
    "- takes a piece of text (in the format of shown below) as an input\n",
    "- extracts data into a list of tuples , e.g. `[('F', 'Ford Motor Company', '15.70', '+0.25', '+1.62%'), ...]`, using regular expression\n",
    "- returns the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.060105Z",
     "start_time": "2021-10-19T07:27:50.568579Z"
    },
    "id": "Cq9Xzhnw-4KD"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.metrics import pairwise_distances\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.070106Z",
     "start_time": "2021-10-19T07:27:54.062104Z"
    },
    "id": "zQjVT1yS-4KF"
   },
   "outputs": [],
   "source": [
    "# columes are: Symbol   Name   Price   Change   % Change\n",
    "text = '''F     Ford Motor Company    15.70     +0.25     +1.62%   \n",
    "            AAPL   Apple Inc. 144.84   +1.08   +0.75%   \n",
    "            SPCE   Virgin Galactic Holdings, Inc.   20.01   -4.05   -16.83%      \n",
    "            BAC    Bank of America Corporation      46.37   +1.30   +2.88%      \n",
    "            WFC    Wells Fargo & Company            48.38   +3.07   +6.78%      \n",
    "            PCG    PG&E Corporation     11.20   +0.43   +3.99%      \n",
    "            T      AT&T Inc.   25.70   +0.08   +0.31%'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.082105Z",
     "start_time": "2021-10-19T07:27:54.072106Z"
    },
    "id": "StFfIbLB-4KG"
   },
   "outputs": [],
   "source": [
    "def extract(text):\n",
    "   \n",
    "    result = None\n",
    "    text=re.findall(r'([A-Z]+)\\s+([\\w.,&\\s]+\\S)\\s+([\\d.]+)\\s+([0-9.+-]+)\\s+([0-9.0-9%.+-]+)',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.094108Z",
     "start_time": "2021-10-19T07:27:54.084106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('F', 'Ford Motor Company', '15.70', '+0.25', '+1.62%'),\n",
       " ('AAPL', 'Apple Inc.', '144.84', '+1.08', '+0.75%'),\n",
       " ('SPCE', 'Virgin Galactic Holdings, Inc.', '20.01', '-4.05', '-16.83%'),\n",
       " ('BAC', 'Bank of America Corporation', '46.37', '+1.30', '+2.88%'),\n",
       " ('WFC', 'Wells Fargo & Company', '48.38', '+3.07', '+6.78%'),\n",
       " ('PCG', 'PG&E Corporation', '11.20', '+0.43', '+3.99%'),\n",
       " ('T', 'AT&T Inc.', '25.70', '+0.08', '+0.31%')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aab-6OWh-4KI"
   },
   "source": [
    "## Q2: Develop a method to summarize a document\n",
    "\n",
    "When you have a long document, you would hope to create a concise summary while preserving it's key information content and overall meaning. Let's implement an `extractive method` based on the concept of TF-IDF. The idea is to identify the key sentences from an article and use them as a summary. Carefully follow the steps below to produce the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsnCv2VZ-4KI"
   },
   "source": [
    "### Q2.1. Preprocess the input document\n",
    "\n",
    "Define a function `proprocess(doc, lemmatized = True, remove_stopword = True, remove_punctuation = True)` \n",
    "- Inputs with four parameters:\n",
    "    - `doc`: an input string (e.g. a document)\n",
    "    - `lemmatized`: an optional boolean parameter to indicate if tokens are lemmatized. The default value is True (i.e. tokens are lemmatized).\n",
    "    - `remove_stopword`: an optional boolean parameter to remove stop words. The default value is True (i.e. remove stop words). \n",
    "    - `remove_punctuation`: optional boolean parameter to remove punctuations. The default values is True (i.e. remove all punctuations)\n",
    "       \n",
    "- Split the input `doc` into sentences\n",
    "- Tokenize each sentence into unigram tokens and also clean up the tokens as follows:\n",
    "    - Remove '-\\n' with regular express\n",
    "    - If `lemmatized` is True, lemmatize all unigrams.\n",
    "    - If `remove_stopword` is set to True, remove all stop words. \n",
    "    - If `remove_punctuation` is set to True, remove all punctuations.\n",
    "    - Convert all tokens to the lower case and remove empty ones\n",
    "\n",
    "- Return the original sentence list (`sents`) and also the tokenized sentence list (`tokenized_sents`). \n",
    "   \n",
    "(Hint: you can use [spacy](https://spacy.io/api/token#attributes) package for this task.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.556104Z",
     "start_time": "2021-10-19T07:27:54.096107Z"
    },
    "id": "A9gBjG6V-4KK"
   },
   "outputs": [],
   "source": [
    "def preprocess(doc, lemmatized=True, remove_stopword=True, remove_punctuation = True):\n",
    "    sents=[]\n",
    "    tokenized_sents1=[]\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    text1=re.sub(r'-\\n','',doc)\n",
    "    doc1 = nlp(text1)\n",
    "    for sent in doc1.sents:\n",
    "        \n",
    "        sent1 = []\n",
    "        if lemmatized==True:\n",
    "            if remove_stopword==True:\n",
    "                if remove_punctuation == True:\n",
    "                    for token in sent:\n",
    "                        if ((not token.is_stop) & (not token.is_punct)&(not token.is_space)):\n",
    "                            sent1.append((token.lemma_ ).lower())\n",
    "        \n",
    "        tokenized_sents1.append(sent1)\n",
    "        sents.append(sent)\n",
    "    tokenized_sents=[token for token in tokenized_sents1 if token!=[]]\n",
    "    \n",
    "\n",
    "    \n",
    "    return sents, tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.566108Z",
     "start_time": "2021-10-19T07:27:54.558107Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kenx0JgK-4KL",
    "outputId": "309988ea-efde-4cd6-ed0b-45ad8a566b26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advances in natural\n",
      "language processing\n",
      "Julia Hirschberg1* and Christopher D. Manning2,3\n",
      "Natural language processing employs computational techniques for the purpose of learning,\n",
      "understanding, and producing human language content. Early computational approaches to\n",
      "language research focused on automating the analysis of the linguistic structure of language\n",
      "and developing basic technologies such as machine translation, speech recognition, and speech\n",
      "synthesis. Today’s researchers refine and make \n"
     ]
    }
   ],
   "source": [
    "# load test document\n",
    "\n",
    "text = open(\"hw1_test_doc.txt\", \"r\", encoding='utf-8').read()\n",
    "print(text[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:56.835401Z",
     "start_time": "2021-10-19T07:27:54.568105Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drwiHlEb-4KL",
    "outputId": "fe258009-9ff4-43c4-eb71-1a2876e6084d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advance', 'natural', 'language', 'processing', 'julia', 'hirschberg1', 'christopher', 'd.', 'manning2,3', 'natural', 'language', 'processing', 'employ', 'computational', 'technique', 'purpose', 'learning', 'understanding', 'produce', 'human', 'language', 'content']\n",
      "['early', 'computational', 'approach', 'language', 'research', 'focus', 'automate', 'analysis', 'linguistic', 'structure', 'language', 'develop', 'basic', 'technology', 'machine', 'translation', 'speech', 'recognition', 'speech', 'synthesis']\n",
      "['today', 'researcher', 'refine', 'use', 'tool', 'real', 'world', 'application', 'create', 'spoken', 'dialogue', 'system', 'speech', 'speech', 'translation', 'engine', 'mine', 'social', 'medium', 'information', 'health', 'finance', 'identify', 'sentiment', 'emotion', 'product', 'service']\n",
      "['describe', 'success', 'challenge', 'rapidly', 'advance', 'area']\n",
      "['past', '20', 'year', 'computational', 'linguistic', 'grow', 'exciting', 'area', 'scientific', 'research', 'practical', 'technology', 'increasingly', 'incorporate', 'consumer', 'product', 'example', 'application', 'apple', 'siri', 'skype', 'translator']\n"
     ]
    }
   ],
   "source": [
    "# test preprocess function\n",
    "sents, tokenized_sents = preprocess(text)\n",
    "\n",
    "for sent in tokenized_sents[0:5]:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFBMtpV7-4KM"
   },
   "source": [
    "### Q2.2. Generate TF-IDF representations for sentences\n",
    "\n",
    "Define a function `compute_tf_idf(sents, use_idf)` as follows: \n",
    "\n",
    "\n",
    "- Take the following two inputs:\n",
    "    - `sents`: tokenized sentences returned from Q2.1. These sentences form a corpus for you to calculate `TF-IDF` vectors.\n",
    "    - `use_idf`: if this option is true, return smoothed normalized `TF_IDF` vectors for all sentences; otherwise, just return normalized `TF` vector for each sentence.\n",
    "    \n",
    "- Calculate `TF-IDF` vectors as shown in the lecture notes (Hint: you can slightly modify code segment 7.5 in NLP Lecture Notes (II) for this task)\n",
    "\n",
    "- Return the `TF-IDF` vectors  if `use_idf` is True.  Return the `TF` vectors if `use_idf` is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:56.846403Z",
     "start_time": "2021-10-19T07:27:56.837402Z"
    },
    "id": "pq9vq7uP-4KM"
   },
   "outputs": [],
   "source": [
    "def compute_tf_idf(sents, use_idf = True):\n",
    "    \n",
    "    tf_idf = None\n",
    "\n",
    "    docs_tokens={idx:{token:sent.count(token) for token in set(sent)} for idx,sent in enumerate(tokenized_sents)}\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1, keepdims=True)\n",
    "    tf=np.divide(tf, doc_len)\n",
    "    df=np.where(tf>0,1,0)\n",
    "    smoothed_idf=np.log(np.divide(len(sents)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=tf*smoothed_idf    \n",
    "    if use_idf == True:\n",
    "        tf_idf=smoothed_tf_idf\n",
    "    else:\n",
    "        tf_idf=tf\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:57.344401Z",
     "start_time": "2021-10-19T07:27:56.848401Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBOzkpEk-4KN",
    "outputId": "8a38a63c-b55c-44f5-b2bf-a3b3b5a7d4c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 1172)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "\n",
    "# show shape of TF-IDF\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYffajS-4KN"
   },
   "source": [
    "### Q2.3. Identify key sentences as summary\n",
    "\n",
    "The basic idea is that, in a coherence article, all sentences should center around some key ideas. If we can identify a subset of sentences, denoted as $S_{key}$, which precisely capture the key ideas,  then $S_{key}$ can be used as a summary. Moreover, $S_{key}$ should have high similarity to all the other sentences on average, because all sentences are centered around the key ideas contained in $S_{key}$. Therefore, we can identify whether a sentence belongs to $S_{key}$ by its similarity to all the other sentences.\n",
    "\n",
    "\n",
    "Define a function `get_summary(tf_idf, sents, topN = 5)`  as follows:\n",
    "\n",
    "- This function takes three inputs:\n",
    "    - `tf_idf`: the TF-IDF vectors of all the sentences in a document\n",
    "    - `sents`: the original sentences corresponding to the TF-IDF vectors\n",
    "    - `topN`: the top N sentences in the generated summary\n",
    "\n",
    "- Steps:\n",
    "    1. Calculate the cosine similarity for every pair of TF-IDF vectors\n",
    "    1. For each sentence, calculate its average similarity to all the others\n",
    "    1. Select the sentences with the `topN` largest average similarity\n",
    "    1. Print the `topN` sentences index\n",
    "    1. Return these sentences as the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:29:47.949165Z",
     "start_time": "2021-10-19T07:29:47.945086Z"
    },
    "id": "gDqgTk8c-4KO"
   },
   "outputs": [],
   "source": [
    "def get_summary(tf_idf, sents, topN = 5):\n",
    "    \n",
    "    summary = None\n",
    "    similarity=1-pairwise_distances(tf_idf, metric = 'cosine')\n",
    "    summ=similarity.sum(axis=0)\n",
    "    average=summ/len(summ)\n",
    "    order=np.argsort(-average)\n",
    "    order_top5=order[0:topN]\n",
    "    summary1=sents[order[0]]\n",
    "    summary2=sents[order[1]]\n",
    "    summary3=sents[order[2]]\n",
    "    summary4=sents[order[3]]\n",
    "    summary5=sents[order[4]]\n",
    "    summary=[summary1,summary2,summary3,summary4,summary5]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:29:50.689152Z",
     "start_time": "2021-10-19T07:29:48.264799Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sw5L48aK-4KO",
    "outputId": "1d5109ef-432f-4cf5-d0b6-46d1b08dc96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early computational approaches to\n",
      "language research focused on automating the analysis of the linguistic structure of language\n",
      "and developing basic technologies such as machine translation, speech recognition, and speech\n",
      "synthesis. \n",
      "\n",
      "Today’s researchers refine and make use of such tools in real-world applications,\n",
      "creating spoken dialogue systems and speech-to-speech translation engines, mining social\n",
      "media for information about health or finance, and identifying sentiment and emotion toward\n",
      "products and services. \n",
      "\n",
      "These efforts illustrate computational\n",
      "approaches to big data, based on current cuttingedge methodologies that combine statistical analysis and ML with knowledge of language. \n",
      "\n",
      "Building large models\n",
      "of this form is much more practical with the\n",
      "massive parallel computation that is now economically available via graphics processing units. \n",
      "\n",
      "2Department of Linguistics, Stanford University,\n",
      "Stanford, CA 94305-2150, USA. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put everything together and test with different options\n",
    "\n",
    "sents, tokenized_sents = preprocess(text)\n",
    "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "summary = get_summary(tf_idf, sents, topN = 5)\n",
    "\n",
    "for sent in summary:\n",
    "    print(sent,\"\\n\")\n",
    "\n",
    "# test with the option lemmatized=False, remove_stopword=False\n",
    "\n",
    "# sents, tokenized_sents = preprocess(text, lemmatized=False, remove_stopword=False, remove_punctuation = True )\n",
    "# tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "# summary = get_summary(tf_idf, sents, topN = 5)\n",
    "# for sent in summary:\n",
    "#    print(sent,\"\\n\")\n",
    "\n",
    "# # test with the option use_idf = False\n",
    "\n",
    "# sents, tokenized_sents = preprocess(text)\n",
    "# tf_idf = compute_tf_idf(tokenized_sents, use_idf = False)\n",
    "# summary = get_summary(tf_idf, sents, topN = 5)\n",
    "# for sent in summary:\n",
    "#    print(sent,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l0bXLQG-4KO"
   },
   "source": [
    "### Q2.4. Analysis\n",
    "\n",
    "- Do you think this method is able to generate a good summary? Any pros or cons have you observed?\n",
    "- Do these options `lemmatized, remove_stopword, remove_punctuation, use_idf` matter? \n",
    "- Why do you think these options matter or do not matter? \n",
    "- If these options matter, what are the best values for these options?\n",
    "\n",
    "\n",
    "Write your analysis as a pdf file. Be sure to provide some evidence from the output of each step to support your arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIyY8lIr-4KP"
   },
   "source": [
    "## Q2.5. (Bonus)\n",
    "\n",
    "\n",
    "- Can you think a way to improve this extractive summary method? Explain the method you propose for improvement,  implement it, use it to generate a new summary, and demonstrate what is improved in the new summary.\n",
    "\n",
    "- Or, you can research on some other extractive summary methods and implement one here. Compare it with the one you implemented in Q2.1-Q2.3 and show pros and cons of each method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The creation of SDSs, whether between hu-\\nmans or between humans and artificial agents,\\nrequires tools for automatic speech recognition\\n(ASR), to identify what a human says; dialogue\\nmanagement (DM), to determine what that hu-\\nman wants; actions to obtain the information or\\nperform the activity requested; and text-to-speech\\n(TTS) synthesis, to convey that information back\\nto the human in spoken form.',\n",
       " 'Computation-\\nal linguistic systems can have multiple purposes:\\nThe goal can be aiding human-human commu-\\nnication, such as in machine translation (MT);\\naiding human-machine communication, such as\\nwith conversational agents; or benefiting both\\nhumans and machines by analyzing and learn-\\ning from the enormous quantity of human lan-\\nguage content that is now available online.',\n",
       " 'Early computational approaches to\\nlanguage research focused on automating the analysis of the linguistic structure of language\\nand developing basic technologies such as machine translation, speech recognition, and speech\\nsynthesis.',\n",
       " 'Social media also provide very large and rich\\nsources of conversational data in Web forums\\nthat can provide “found”data for the study of lan-\\nguage phenomena such as code-switching (mixed\\nlanguage in bilingual speech), hedging behavior\\n(words and phrases indicating lack of commitment\\nto a proposition such as “sort of ”), and hate speech\\nor bullying behavior.',\n",
       " 'Social media\\nsuch as Twitter, blog posts, and forums also provide\\nresearchers with very large amounts of data to use in\\nassessing the role of sentiment and emotion in\\nidentifying other linguistic or social phenomena\\n[e.g., sarcasm (63), power relationships, and so-\\ncial influence (64)], as well as mental health is- \\nsues [e.g., depression (65)].']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sents = nltk.sent_tokenize(text)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sents)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "summary=cosine_sim.sum(axis=0)\n",
    "average=summary/len(summary)\n",
    "order=np.argsort(-average)\n",
    "summary1=sents[order[0]]\n",
    "summary2=sents[order[1]]\n",
    "summary3=sents[order[2]]\n",
    "summary4=sents[order[3]]\n",
    "summary5=sents[order[4]]\n",
    "summaryall=[summary1,summary2,summary3,summary4,summary5]\n",
    "summaryall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['early',\n",
       " 'computational',\n",
       " 'approach',\n",
       " 'language',\n",
       " 'research',\n",
       " 'focus',\n",
       " 'automate',\n",
       " 'analysis',\n",
       " 'linguistic',\n",
       " 'structure',\n",
       " 'language',\n",
       " 'develop',\n",
       " 'basic',\n",
       " 'technology',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'speech',\n",
       " 'synthesis']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'researcher',\n",
       " 'refine',\n",
       " 'use',\n",
       " 'tool',\n",
       " 'real',\n",
       " 'world',\n",
       " 'application',\n",
       " 'create',\n",
       " 'spoken',\n",
       " 'dialogue',\n",
       " 'system',\n",
       " 'speech',\n",
       " 'speech',\n",
       " 'translation',\n",
       " 'engine',\n",
       " 'mine',\n",
       " 'social',\n",
       " 'medium',\n",
       " 'information',\n",
       " 'health',\n",
       " 'finance',\n",
       " 'identify',\n",
       " 'sentiment',\n",
       " 'emotion',\n",
       " 'product',\n",
       " 'service']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['computational',\n",
       " 'linguistic',\n",
       " 'know',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'subfield',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'concern',\n",
       " 'computational',\n",
       " 'technique',\n",
       " 'learn',\n",
       " 'understand',\n",
       " 'produce',\n",
       " 'human',\n",
       " 'language',\n",
       " 'content']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['creation',\n",
       " 'sds',\n",
       " 'human',\n",
       " 'human',\n",
       " 'artificial',\n",
       " 'agent',\n",
       " 'require',\n",
       " 'tool',\n",
       " 'automatic',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'asr',\n",
       " 'identify',\n",
       " 'human',\n",
       " 'say',\n",
       " 'dialogue',\n",
       " 'management',\n",
       " 'dm',\n",
       " 'determine',\n",
       " 'human',\n",
       " 'want',\n",
       " 'action',\n",
       " 'obtain',\n",
       " 'information',\n",
       " 'perform',\n",
       " 'activity',\n",
       " 'request',\n",
       " 'text',\n",
       " 'speech',\n",
       " 'tts',\n",
       " 'synthesis',\n",
       " 'convey',\n",
       " 'information',\n",
       " 'human',\n",
       " 'spoken',\n",
       " 'form']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['subsequent',\n",
       " 'research',\n",
       " 'aim',\n",
       " 'well',\n",
       " 'exploit',\n",
       " 'structure',\n",
       " 'human',\n",
       " 'language',\n",
       " 'sentence',\n",
       " 'i.e.',\n",
       " 'syntax',\n",
       " 'translation',\n",
       " 'system',\n",
       " '7',\n",
       " '8)',\n",
       " 'researcher',\n",
       " 'actively',\n",
       " 'build',\n",
       " 'deeply',\n",
       " 'meaning',\n",
       " 'representation',\n",
       " 'language',\n",
       " '9',\n",
       " 'enable',\n",
       " 'new',\n",
       " 'level',\n",
       " 'semantic',\n",
       " 'mt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sents, tokenized_sents = preprocess(text)\n",
    "docs_tokens={idx:{token:sent.count(token) for token in set(sent)} for idx,sent in enumerate(tokenized_sents)}\n",
    "dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "dtm=dtm.fillna(0)\n",
    "dtm = dtm.sort_index(axis = 0)\n",
    "tf=dtm.values\n",
    "doc_len=tf.sum(axis=1, keepdims=True)\n",
    "tf=np.divide(tf, doc_len)\n",
    "df=np.where(tf>0,1,0)\n",
    "smoothed_idf=np.log(np.divide(len(sents)+1, np.sum(df, axis=0)+1))+1 \n",
    "smoothed_tf_idf=tf*smoothed_idf  \n",
    "tf_idf=smoothed_tf_idf\n",
    "cosine_sim = cosine_similarity(tf_idf, tf_idf)\n",
    "nx_graph = nx.from_numpy_array(cosine_sim)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(tokenized_sents)), reverse=True)\n",
    "ranked_sentences[0][1]\n",
    "ranked_sentences[1][1]\n",
    "ranked_sentences[2][1]\n",
    "ranked_sentences[3][1]\n",
    "ranked_sentences[4][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:29:53.023232Z",
     "start_time": "2021-10-19T07:29:50.691147Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEVWwkRW-4KP",
    "outputId": "cf1d3b08-f8e9-4332-b0ee-2d43b01ecfac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      "\n",
      "Test Q1\n",
      "[('F', 'Ford Motor Company', '15.70', '+0.25', '+1.62%'), ('AAPL', 'Apple Inc.', '144.84', '+1.08', '+0.75%'), ('SPCE', 'Virgin Galactic Holdings, Inc.', '20.01', '-4.05', '-16.83%'), ('BAC', 'Bank of America Corporation', '46.37', '+1.30', '+2.88%'), ('WFC', 'Wells Fargo & Company', '48.38', '+3.07', '+6.78%'), ('PCG', 'PG&E Corporation', '11.20', '+0.43', '+3.99%'), ('T', 'AT&T Inc.', '25.70', '+0.08', '+0.31%')]\n",
      "\n",
      "==================\n",
      "\n",
      "Test Q2\n",
      "Early computational approaches to\n",
      "language research focused on automating the analysis of the linguistic structure of language\n",
      "and developing basic technologies such as machine translation, speech recognition, and speech\n",
      "synthesis. \n",
      "\n",
      "Today’s researchers refine and make use of such tools in real-world applications,\n",
      "creating spoken dialogue systems and speech-to-speech translation engines, mining social\n",
      "media for information about health or finance, and identifying sentiment and emotion toward\n",
      "products and services. \n",
      "\n",
      "These efforts illustrate computational\n",
      "approaches to big data, based on current cuttingedge methodologies that combine statistical analysis and ML with knowledge of language. \n",
      "\n",
      "Building large models\n",
      "of this form is much more practical with the\n",
      "massive parallel computation that is now economically available via graphics processing units. \n",
      "\n",
      "2Department of Linguistics, Stanford University,\n",
      "Stanford, CA 94305-2150, USA. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Q1\n",
    "    \n",
    "    text='''F   Ford Motor Company  15.70   +0.25   +1.62%   \n",
    "AAPL   Apple Inc.   144.84   +1.08   +0.75%   \n",
    "SPCE   Virgin Galactic Holdings, Inc.   20.01   -4.05   -16.83%      \n",
    "BAC   Bank of America Corporation   46.37   +1.30   +2.88%      \n",
    "WFC   Wells Fargo & Company   48.38   +3.07   +6.78%      \n",
    "PCG   PG&E Corporation   11.20   +0.43   +3.99%      \n",
    "T   AT&T Inc.   25.70   +0.08   +0.31%'''\n",
    "    \n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q1\")\n",
    "    print(extract(text))\n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2\")\n",
    "    \n",
    "    text = open(\"hw1_test_doc.txt\", \"r\", encoding='utf-8').read()\n",
    "    sents, tokenized_sents = preprocess(text)\n",
    "    tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "    summary = get_summary(tf_idf, sents, topN = 5)\n",
    "    \n",
    "    for sent in summary:\n",
    "        print(sent,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
